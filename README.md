# Åšrodowisko Fine-Tuningu Modeli z LM Studio

Kompletne Å›rodowisko do fine-tuningu modeli jÄ™zykowych zintegrowane z LM Studio. Zawiera wszystkie niezbÄ™dne narzÄ™dzia do przygotowania danych, treningu modeli i ich ewaluacji.

## ğŸš€ Szybki Start

### 1. Instalacja

```bash
# Sklonuj lub pobierz repozytorium
git clone <repo-url>
cd fine_tuning_sample

# Zainstaluj zaleÅ¼noÅ›ci
pip install -r requirements.txt

# Skonfiguruj Å›rodowisko
python scripts/setup_environment.py
```

### 2. Przygotowanie danych

```bash
# UtwÃ³rz przykÅ‚adowy dataset
python scripts/data_preparation.py create-sample --output data/train.jsonl --num-samples 100

# Waliduj dane
python scripts/data_preparation.py validate --input data/train.jsonl

# Podziel na train/val/test
python scripts/data_preparation.py split --input data/train.jsonl
```

### 3. Fine-tuning

```bash
# Uruchom fine-tuning
python scripts/fine_tune.py --config configs/training_config.yaml

# Lub z wÅ‚asnymi parametrami
python scripts/fine_tune.py --data data/train.jsonl --epochs 5 --lr 1e-5
```

### 4. Ewaluacja

```bash
# Testuj model z LM Studio
python scripts/lm_studio_client.py test --model "twÃ³j-model" --prompt "Test prompt"

# Ewaluuj na datasetcie
python scripts/lm_studio_client.py evaluate --model "twÃ³j-model" --dataset data/test.jsonl --output results.json

# Analizuj wyniki
python scripts/model_evaluation.py evaluate --results results.json --output-dir output/evaluation
```

## ğŸ“ Struktura Projektu

```
fine_tuning_sample/
â”œâ”€â”€ configs/                 # Pliki konfiguracyjne
â”‚   â”œâ”€â”€ training_config.yaml # Konfiguracja treningu
â”‚   â””â”€â”€ lm_studio_config.json # Konfiguracja LM Studio
â”œâ”€â”€ data/                    # Dane treningowe
â”‚   â”œâ”€â”€ raw/                 # Surowce dane
â”‚   â””â”€â”€ processed/           # Przetworzone dane
â”œâ”€â”€ models/                  # Modele
â”‚   â”œâ”€â”€ base/                # Modele bazowe
â”‚   â””â”€â”€ fine_tuned/          # Wytrenowane modele
â”œâ”€â”€ scripts/                 # Skrypty pomocnicze
â”‚   â”œâ”€â”€ setup_environment.py # Konfiguracja Å›rodowiska
â”‚   â”œâ”€â”€ fine_tune.py         # GÅ‚Ã³wny skrypt treningu
â”‚   â”œâ”€â”€ lm_studio_client.py  # Klient LM Studio
â”‚   â”œâ”€â”€ data_preparation.py  # Przygotowanie danych
â”‚   â””â”€â”€ model_evaluation.py  # Ewaluacja modeli
â”œâ”€â”€ output/                  # Wyniki
â”‚   â”œâ”€â”€ checkpoints/         # Checkpointy treningu
â”‚   â””â”€â”€ logs/                # Logi
â”œâ”€â”€ logs/                    # Logi systemowe
â”œâ”€â”€ requirements.txt         # ZaleÅ¼noÅ›ci Python
â””â”€â”€ README.md               # Dokumentacja
```

## âš™ï¸ Konfiguracja

### Plik `configs/training_config.yaml`

GÅ‚Ã³wny plik konfiguracyjny zawiera:

- **Model**: nazwa, Å›cieÅ¼ka, parametry
- **Trening**: epochs, batch size, learning rate
- **LoRA**: konfiguracja adaptacji
- **Optymalizacja pamiÄ™ci**: fp16, gradient checkpointing
- **Dane**: Å›cieÅ¼ki do datasetÃ³w
- **Logowanie**: wandb, tensorboard

### Plik `configs/lm_studio_config.json`

Konfiguracja integracji z LM Studio:

- **API**: URL, timeout, retry
- **Modele**: lista dostÄ™pnych modeli
- **Formaty**: szablony instrukcji i odpowiedzi

## ğŸ“Š Format Danych

Dane treningowe w formacie JSONL:

```json
{"instruction": "WyjaÅ›nij czym jest AI", "response": "Sztuczna inteligencja to..."}
{"instruction": "Jak gotowaÄ‡ makaron?", "response": "1. Zagotuj wodÄ™..."}
```

### Konwersja z innych formatÃ³w

```bash
# CSV â†’ JSONL
python scripts/data_preparation.py convert-csv --input data.csv --output data.jsonl

# TXT â†’ JSONL
python scripts/data_preparation.py convert-txt --input data.txt --output data.jsonl
```

## ğŸ”§ NarzÄ™dzia

### 1. Przygotowanie Danych (`scripts/data_preparation.py`)

- Konwersja formatÃ³w (CSV, TXT â†’ JSONL)
- Walidacja datasetÃ³w
- Filtrowanie wedÅ‚ug dÅ‚ugoÅ›ci
- PodziaÅ‚ na train/val/test
- Tworzenie przykÅ‚adowych danych

### 2. Fine-Tuning (`scripts/fine_tune.py`)

- Trening z LoRA
- Optymalizacja pamiÄ™ci
- Monitorowanie z TensorBoard
- Zapis checkpointÃ³w
- Integracja z Transformers

### 3. Klient LM Studio (`scripts/lm_studio_client.py`)

- Komunikacja z API LM Studio
- Testowanie modeli
- Ewaluacja na datasetach
- Generowanie odpowiedzi

### 4. Ewaluacja (`scripts/model_evaluation.py`)

- Metryki BLEU i ROUGE
- Analiza czasÃ³w odpowiedzi
- Wizualizacja wynikÃ³w
- PorÃ³wnanie modeli
- Generowanie raportÃ³w

## ğŸ¯ PrzykÅ‚ady UÅ¼ycia

### Fine-tuning modelu Llama-2

```bash
# 1. Przygotuj dane
python scripts/data_preparation.py create-sample --output data/llama_data.jsonl --num-samples 500

# 2. Skonfiguruj model w configs/training_config.yaml
# model:
#   name: "llama-2-7b-chat"
#   path: "./models/"

# 3. Uruchom trening
python scripts/fine_tune.py --config configs/training_config.yaml

# 4. Przetestuj model
python scripts/lm_studio_client.py test --model "llama-2-7b-chat"
```

### Ewaluacja wielu modeli

```bash
# Ewaluuj modele
python scripts/lm_studio_client.py evaluate --model "model-1" --dataset data/test.jsonl --output results1.json
python scripts/lm_studio_client.py evaluate --model "model-2" --dataset data/test.jsonl --output results2.json

# PorÃ³wnaj wyniki
python scripts/model_evaluation.py compare --compare-results results1.json results2.json --compare-output comparison.csv
```

### Optymalizacja pamiÄ™ci

```yaml
# W configs/training_config.yaml
memory_optimization:
  gradient_checkpointing: true
  fp16: false
  bf16: true
  use_8bit_optimizer: true

lora:
  enabled: true
  r: 8  # Mniejsza wartoÅ›Ä‡ = mniej pamiÄ™ci
```

## ğŸ“ˆ Monitorowanie

### TensorBoard

```bash
# Uruchom TensorBoard
tensorboard --logdir output/logs

# OtwÃ³rz http://localhost:6006
```

### Logi

Logi zapisywane w:
- `logs/training/fine_tune.log` - logi treningu
- `output/logs/` - logi TensorBoard
- `logs/` - logi systemowe

## ğŸ› RozwiÄ…zywanie ProblemÃ³w

### BÅ‚Ä…d: "CUDA out of memory"

```yaml
# Zmniejsz batch size
training:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8

# WÅ‚Ä…cz gradient checkpointing
memory_optimization:
  gradient_checkpointing: true
```

### BÅ‚Ä…d: "Model not found"

1. SprawdÅº Å›cieÅ¼kÄ™ w `configs/training_config.yaml`
2. Upewnij siÄ™, Å¼e model jest w katalogu `models/`
3. SprawdÅº format modelu (GGUF dla LM Studio)

### BÅ‚Ä…d: "LM Studio connection failed"

1. Uruchom LM Studio
2. ZaÅ‚aduj model
3. SprawdÅº port 1234
4. SprawdÅº konfiguracjÄ™ w `configs/lm_studio_config.json`

## ğŸ“š Dokumentacja Dodatkowa

### Metryki Ewaluacji

- **BLEU**: JakoÅ›Ä‡ generowanego tekstu (0-1)
- **ROUGE**: Pokrycie informacji (0-1)
- **Czas odpowiedzi**: WydajnoÅ›Ä‡ modelu
- **Stosunek dÅ‚ugoÅ›ci**: PorÃ³wnanie z oczekiwaniami

### Optymalizacja WydajnoÅ›ci

1. **LoRA**: Zmniejsza liczbÄ™ parametrÃ³w do trenowania
2. **Gradient Checkpointing**: OszczÄ™dza pamiÄ™Ä‡
3. **Mixed Precision**: fp16/bf16 dla szybszego treningu
4. **DataLoader**: Optymalizacja Å‚adowania danych

## ğŸ¤ WkÅ‚ad w Projekt

1. Fork repozytorium
2. UtwÃ³rz branch dla funkcji
3. Commit zmian
4. Push do branch
5. UtwÃ³rz Pull Request

## ğŸ“„ Licencja

MIT License - zobacz plik LICENSE dla szczegÃ³Å‚Ã³w.

## ğŸ”— Linki

- [LM Studio](https://lmstudio.ai/)
- [Transformers](https://huggingface.co/transformers/)
- [PEFT](https://github.com/huggingface/peft)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

## ğŸ“ Wsparcie

W przypadku problemÃ³w:
1. SprawdÅº sekcjÄ™ "RozwiÄ…zywanie ProblemÃ³w"
2. Przejrzyj logi w katalogu `logs/`
3. UtwÃ³rz issue w repozytorium
4. Opisz problem i doÅ‚Ä…cz logi

---

**Powodzenia w fine-tuningu! ğŸš€**
